{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN6k8zOwKRdn4OI9cy9Svqh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ishikaaa/PDF-extraction/blob/main/PDF_extraction_falcon_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhIGZm0lc0mw"
      },
      "outputs": [],
      "source": [
        "# install libraries\n",
        "!pip install PyPDF2\n",
        "!pip install langchain\n",
        "!pip install InstructorEmbedding\n",
        "!pip install sentence-transformers==2.2.2\n",
        "!pip install faiss-gpu\n",
        "!pip install -U langchain-community\n",
        "!pip install python-docx"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms import HuggingFaceHub\n",
        "import re\n",
        "import os\n",
        "import time\n",
        "\n",
        "from PyPDF2 import PdfReader\n",
        "from docx import Document\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "37Xxf732dQ6X"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_pdf_text(pdf_path):\n",
        "    \"\"\"\n",
        "    Extracts text from a PDF file.\n",
        "\n",
        "    Args:\n",
        "        pdf_path (str): Path to the PDF file.\n",
        "\n",
        "    Returns:\n",
        "        str: Text extracted from the PDF file.\n",
        "    \"\"\"\n",
        "    text = \"\"\n",
        "    pdf_reader = PdfReader(pdf_path)\n",
        "    for page in pdf_reader.pages:\n",
        "        text += page.extract_text() or \"\"\n",
        "    return text\n",
        "\n",
        "def extract_docx_text(docx_path):\n",
        "    \"\"\"\n",
        "    Extracts text from a DOCX file.\n",
        "\n",
        "    Args:\n",
        "        docx_path (str): Path to the DOCX file.\n",
        "\n",
        "    Returns:\n",
        "        str: Text extracted from the DOCX file.\n",
        "    \"\"\"\n",
        "    text = \"\"\n",
        "    doc = Document(docx_path)\n",
        "    for para in doc.paragraphs:\n",
        "        text += para.text + \"\\n\"\n",
        "    return text\n",
        "\n",
        "\n",
        "def extract_txt_text(txt_path):\n",
        "    \"\"\"\n",
        "    Extracts text from a TXT file.\n",
        "\n",
        "    Args:\n",
        "        txt_path (str): Path to the TXT file.\n",
        "\n",
        "    Returns:\n",
        "        str: Text extracted from the TXT file.\n",
        "    \"\"\"\n",
        "    with open(txt_path, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "7_x1i9hLyj7_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step-1 Load PDF\n",
        "def extract_text(file_path):\n",
        "    \"\"\"\n",
        "    Extracts text from a file (PDF, DOCX, TXT).\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the file.\n",
        "\n",
        "    Returns:\n",
        "        str: Text extracted from the file.\n",
        "    \"\"\"\n",
        "    ext = Path(file_path).suffix.lower()\n",
        "    if ext == '.pdf':\n",
        "        return extract_pdf_text(file_path)\n",
        "    elif ext == '.docx':\n",
        "        return extract_docx_text(file_path)\n",
        "    elif ext == '.txt':\n",
        "        return extract_txt_text(file_path)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported file format: {ext}\")\n",
        "\n",
        "\n",
        "# Step-2: get the text chunks\n",
        "def get_text_chunks(text):\n",
        "    \"\"\"\n",
        "    : return\n",
        "        a list of chunks of text that we will feed to our model\n",
        "    \"\"\"\n",
        "\n",
        "    text_splitter = CharacterTextSplitter(\n",
        "        separator=\"\\n\",\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=200,\n",
        "        length_function=len,\n",
        "    )\n",
        "    chunks = text_splitter.split_text(text)\n",
        "    return chunks\n",
        "\n",
        "\n",
        "# Step-3: Text Embedding and Create Vector store\n",
        "def get_vectorstore(text_chunks):\n",
        "    embeddings = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-xl\")\n",
        "    vectorstore = FAISS.from_texts(texts=text_chunks, embedding=embeddings)\n",
        "    return embeddings, vectorstore\n",
        "\n",
        "\n",
        "# Step-4: Train the model\n",
        "def retrieval_qa_chain(db, return_source_documents):\n",
        "    llm = HuggingFaceHub(repo_id=\"tiiuae/falcon-7b-instruct\", model_kwargs={\"temperature\": 0.2, \"max_length\": 500, \"max_new_tokens\": 700})\n",
        "    qa_chain = RetrievalQA.from_chain_type(llm=llm,\n",
        "                                           chain_type='stuff',\n",
        "                                           retriever=db,\n",
        "                                           return_source_documents=return_source_documents,\n",
        "                                           )\n",
        "    return qa_chain\n"
      ],
      "metadata": {
        "id": "X9ad-eqoeEVK"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\"\n",
        "\n",
        "    ## Step-1 Load PDF\n",
        "    # pdf_docs = [\"HPOODataSheet.pdf\"]\n",
        "    pdf_docs = \"Ishika_Resume.docx\"\n",
        "    # pdf_docs = \"code_documentation.docx\"\n",
        "    raw_text = extract_text(pdf_docs)\n",
        "\n",
        "    ## Step-2: get the text chunks\n",
        "    text_chunks = get_text_chunks(raw_text)\n",
        "\n",
        "    ## Step-3\n",
        "    embeddings, vector_store = get_vectorstore(text_chunks)\n",
        "\n",
        "    ## Step-4\n",
        "    db = vector_store.as_retriever(search_kwargs={'k': 3})\n",
        "    bot = retrieval_qa_chain(db, True)\n"
      ],
      "metadata": {
        "id": "1QZXSn_leVMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Step-5\n",
        "while True:\n",
        "    query = input(\"Please enter your response (type 'quit' to exit): \")\n",
        "    if query.lower() == 'quit':\n",
        "        print(\"Goodbye!\")\n",
        "        break\n",
        "    else:\n",
        "        start_time = time.time()\n",
        "        sol = bot(query)\n",
        "        end_time = time.time()\n",
        "        answer = sol[\"result\"].split('\\nHelpful Answer:')[-1].strip()\n",
        "        print(\"Question: \", query)\n",
        "        print(\"Answer: \", answer)\n",
        "        print(\"Time: \", end_time - start_time)\n",
        "        print(\"**************************\")"
      ],
      "metadata": {
        "id": "ZwjekaVrgXDC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}