The code is based on a retrieval-based question-answering algorithm, which is able to read PDFs and answer your queries based on the content of those PDFs.
Steps involved in PDF Extraction – 
1.	Extract text from PDFs – 
Purpose - To read one OR more PDF files, extract only the text content (ignoring images OR other non-text elements) from each PDF.
Function used – 
•	PyPDF2.PdfReader() - is used to open and extract text (ignoring non-text elements) from PDF files.
2.	Break text into smaller chunks – 
Purpose - To split the extracted text into smaller chunks so that it can be easily processed by the model.
Function used – 
•	CharacterTextSplitter() – This function is used to split the text into smaller chunks based on a separator (newline), ensuring each chunk has a specified maximum length (1000 characters) and includes some overlap (200 characters) to maintain context.
3.	Text embedding and create vector store – 
Purpose - Perform embedding (i.e., convert text chunks into numerical vectors) and store these embeddings in a vector store for efficient information retrieval and semantic search.
Function used – 
•	HuggingFaceInstructEmbeddings() – This function utilizes the pre-trained model 'hkunlp/instructor-xl' to create high-quality text embeddings. 'hkunlp/instructor-xl' model is specifically designed to produce text embeddings which enhance our model's ability to understand the context and meaning of the text.
•	FAISS() - FAISS stands for 'Facebook AI Similarity Search' (developed by Facebook AI Research). FAISS is used for storing vectors efficiently, enabling similarity search and clustering of dense vectors (grouping similar vectors together based on their features or characteristics).
4.	Train Model – 
Purpose - Prepare a question-answering system that can generate responses based on user queries by configuring and fine-tuning a language model.
Function used – 
•	HuggingFaceHub() – This function is used to initialize and configure pre-trained language model from Hugging Face's model repository. The specific model used in the code is "tiiuae/falcon-7b-instruct" which is pre-trained on a diverse range of text for various NLP tasks, such as text generation, summarization, question answering. In our code, the model is fine-tuned using the following hyperparameters to control the style and length of the generated responses: 
-	Temperature – It controls the randomness of the generated responses, with lower values, it produces more focused answer
-	max_length – It Sets the maximum length of the generated response
-	max_new_tokens – It defines the maximum number of new tokens to be generated by the model.
•	RetrievalQA() – This function creates a question-answering chain that uses the vector store (db) and a pre-trained language model ('llm', initialized with HuggingFaceHub()) to retrieve relevant answers based on user queries.
-	chain_type
-	return_source_documents
5.	Handle User Queries - interactive loop for answering user questions until 'quit' is entered.
