{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM7LB0+raXPugCPAI6Y0zvn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ishikaaa/PDF-extraction/blob/main/PDF_extraction_fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74-5hFbvaJux"
      },
      "outputs": [],
      "source": [
        "!pip install PyPDF2\n",
        "!pip install langchain\n",
        "!pip install InstructorEmbedding\n",
        "!pip install sentence-transformers==2.2.2\n",
        "!pip install faiss-gpu\n",
        "!pip install -U langchain-community\n",
        "!pip install datasets\n",
        "!pip install transformers\n",
        "!pip install accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
      ],
      "metadata": {
        "id": "zsUWwfdeahcT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "# from langchain.llms import HuggingFacePipeline\n",
        "# from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, pipeline\n",
        "\n",
        "# Step-1: get text from single OR multiple PDFs\n",
        "def get_pdf_text(pdf_docs):\n",
        "    \"\"\"\n",
        "    args:\n",
        "        pdf_docs: list of pdfs\n",
        "    \"\"\"\n",
        "    text = \"\"\n",
        "\n",
        "    # iterate through all pdfs\n",
        "    for pdf in pdf_docs:\n",
        "        pdf_reader = PdfReader(pdf)\n",
        "        # iterate through all pages\n",
        "        for page in pdf_reader.pages:\n",
        "            text += page.extract_text()\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "# Step-2: get the text chunks\n",
        "def get_text_chunks(text):\n",
        "    \"\"\"\n",
        "    : return\n",
        "        a list of chunks of text that we will feed to our model\n",
        "    \"\"\"\n",
        "    text_splitter = CharacterTextSplitter(\n",
        "        separator=\"\\n\",\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=200,\n",
        "        length_function=len,\n",
        "    )\n",
        "    chunks = text_splitter.split_text(text)\n",
        "    return chunks\n",
        "\n",
        "\n",
        "# Step-3: Create Vector store\n",
        "def get_vectorstore(text_chunks):\n",
        "    # instructor embeddings\n",
        "    embeddings = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-xl\")\n",
        "    vectorstore = FAISS.from_texts(texts=text_chunks, embedding=embeddings)\n",
        "    return vectorstore\n",
        "\n",
        "\n",
        "def train_model(model_name):\n",
        "    model_name = \"t5-large\"\n",
        "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "    # Create a pipeline for text2text generation\n",
        "    generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
        "    return generator\n",
        "\n",
        "\n",
        "def step5(vector_store, generator, questions):\n",
        "    for question in questions:\n",
        "        docs = vector_store.similarity_search(question)\n",
        "        context = docs[0].page_content\n",
        "\n",
        "        # Combine the question and context for T5\n",
        "        input_text = f\"question: {question} context: {context}. Provide a detailed answer.\"\n",
        "\n",
        "        # Generate the answer with specific parameters\n",
        "        result = generator(input_text, max_length=150, num_beams=4, early_stopping=True)\n",
        "\n",
        "        # Decode the generated text\n",
        "        answer = result[0]['generated_text']\n",
        "\n",
        "        print(f\"Context: {context}\")\n",
        "        print(f\"Question: {question}\")\n",
        "        print(f\"Answer: {answer}\")"
      ],
      "metadata": {
        "id": "Oxj7x5mJa0Ir"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Step-1 Load PDF\n",
        "    pdf_docs = [\"NIPS-2017-attention-is-all-you-need-Paper.pdf\"]\n",
        "    raw_text = get_pdf_text(pdf_docs)\n",
        "\n",
        "    # Step-2: get the text chunks\n",
        "    text_chunks = get_text_chunks(raw_text)\n",
        "\n",
        "    # Step-3\n",
        "    vector_store = get_vectorstore(text_chunks)\n",
        "    print(\"text_chunks: \", vector_store)\n",
        "\n",
        "    # Step-4\n",
        "    # # model_name = \"t5-large\"\n",
        "    # model_name = \"t5-small\"\n",
        "    # generator = train_model(model_name)\n",
        "\n",
        "    # # Step-5\n",
        "    # questions = [\"Who is Aidan N. Gomez\", \"What is encoder and decoder?\"]\n",
        "    # step5(questions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0aKZtPaa1z4",
        "outputId": "11ffe293-5a61-4ed0-89da-183e0d5ad1f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "load INSTRUCTOR_Transformer\n",
            "max_seq_length  512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments, pipeline, DataCollatorForSeq2Seq\n",
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "def fine_tune_model(train_texts, model_name=\"t5-large\"):\n",
        "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "    # Tokenize the dataset\n",
        "    def preprocess_data(examples):\n",
        "        inputs = examples['input_text']\n",
        "        targets = examples['target_text']\n",
        "        model_inputs = tokenizer(inputs, max_length=512, truncation=True)\n",
        "        labels = tokenizer(targets, max_length=512, truncation=True)\n",
        "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "        return model_inputs\n",
        "\n",
        "    # Split the train_texts into training and validation sets (80/20 split)\n",
        "    split_index = int(0.8 * len(train_texts))\n",
        "    train_texts_split = train_texts[:split_index]\n",
        "    val_texts_split = train_texts[split_index:]\n",
        "\n",
        "    # Prepare datasets\n",
        "    train_dataset = Dataset.from_dict({\"input_text\": train_texts_split, \"target_text\": train_texts_split})\n",
        "    val_dataset = Dataset.from_dict({\"input_text\": val_texts_split, \"target_text\": val_texts_split})\n",
        "\n",
        "    train_dataset = train_dataset.map(preprocess_data, batched=True)\n",
        "    val_dataset = val_dataset.map(preprocess_data, batched=True)\n",
        "\n",
        "    # Define training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='./results',\n",
        "        overwrite_output_dir=True,\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=4,\n",
        "        per_device_eval_batch_size=4,\n",
        "        warmup_steps=500,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir='./logs',\n",
        "        logging_steps=10,\n",
        "        evaluation_strategy=\"epoch\"\n",
        "    )\n",
        "\n",
        "    # Data collator\n",
        "    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "\n",
        "    # Initialize Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        data_collator=data_collator\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    trainer.train()\n",
        "\n",
        "    # Save the model\n",
        "    model.save_pretrained('./fine_tuned_model')\n",
        "    tokenizer.save_pretrained('./fine_tuned_model')\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "def create_generator(model_name):\n",
        "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "    generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
        "    return generator\n"
      ],
      "metadata": {
        "id": "-IoN0kurbUAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Fine-tune the model\n",
        "model_name = \"t5-large\"\n",
        "model, tokenizer = fine_tune_model(text_chunks, model_name)\n",
        "\n",
        "# Step 5: Create generator pipeline\n",
        "generator = create_generator('./fine_tuned_model')\n",
        "\n",
        "# Step 6\n"
      ],
      "metadata": {
        "id": "jiusGymBbYEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [\"Who is Aidan N. Gomez\", \"What is encoder and decoder stacks?\", \"What did Ying Cao do?\"]\n",
        "step5(vector_store, generator, questions)"
      ],
      "metadata": {
        "id": "o1oC1S-egayg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}