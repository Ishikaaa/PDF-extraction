{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMATkYo21PW1ZBisj2hoXvx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ishikaaa/PDF-extraction/blob/main/PDF_extraction_fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EapZgB9K2QIi"
      },
      "outputs": [],
      "source": [
        "!pip install PyPDF2\n",
        "!pip install langchain\n",
        "!pip install InstructorEmbedding\n",
        "!pip install sentence-transformers==2.2.2\n",
        "!pip install faiss-gpu\n",
        "!pip install -U langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PyPDF2 import PdfReader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "# from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, pipeline\n",
        "import re\n",
        "# import pickle\n",
        "# import os\n",
        "import time"
      ],
      "metadata": {
        "id": "k1I-G7oWn__M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step-1: get text from single OR multiple PDFs\n",
        "def get_pdf_text(pdf_docs):\n",
        "    \"\"\"\n",
        "    args:\n",
        "        pdf_docs: list of pdfs\n",
        "    \"\"\"\n",
        "    text = \"\"\n",
        "\n",
        "    # iterate through all pdfs\n",
        "    for pdf in pdf_docs:\n",
        "        pdf_reader = PdfReader(pdf)\n",
        "        # iterate through all pages\n",
        "        for page in pdf_reader.pages:\n",
        "            text += page.extract_text()\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "# Step-2: Clean the raw text\n",
        "def clean_context(context):\n",
        "    # Remove non-text elements\n",
        "    # context = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', '', context)  # Remove email addresses\n",
        "    # context = re.sub(r'\\bhttps?:\\/\\/\\S+\\b', '', context)  # Remove URLs\n",
        "    # context = re.sub(r'\\b\\d+\\b', '', context)  # Remove standalone digits\n",
        "    # # Add more regex patterns to remove other non-text elements\n",
        "\n",
        "    # Normalize whitespace\n",
        "    context = re.sub(r'\\s+', ' ', context)\n",
        "    # Lowercase text\n",
        "    context = context.lower()\n",
        "\n",
        "    return context\n",
        "\n",
        "\n",
        "# Step-3: get the text chunks\n",
        "def get_text_chunks(text):\n",
        "    \"\"\"\n",
        "    return:\n",
        "        a list of chunks of text that we will feed to our model\n",
        "    \"\"\"\n",
        "\n",
        "    text_splitter = CharacterTextSplitter(\n",
        "        separator=\"\\n\",\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=200,\n",
        "        length_function=len,\n",
        "    )\n",
        "    chunks = text_splitter.split_text(text)\n",
        "    return chunks\n",
        "\n",
        "\n",
        "# Step-4: Create Vector store\n",
        "def get_vectorstore(text_chunks):\n",
        "    # instructor embeddings\n",
        "    embeddings = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-xl\")\n",
        "    vectorstore = FAISS.from_texts(texts=text_chunks, embedding=embeddings)\n",
        "    return vectorstore\n",
        "\n",
        "\n",
        "def train_model(model_name):\n",
        "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "    # Create a pipeline for text2text generation\n",
        "    generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "    return generator\n",
        "\n",
        "\n",
        "def step6(vector_store, generator, question):\n",
        "    begin = time.time()\n",
        "    docs = vector_store.similarity_search(question)\n",
        "    # context = docs[0].page_content\n",
        "    context = \" \".join([doc.page_content for doc in docs])\n",
        "\n",
        "    # Combine the question and context for T5\n",
        "    input_text = f\"question: {question} context: {context}. Provide a detailed answer.\"\n",
        "\n",
        "    # Generate the answer with specific parameters\n",
        "    # result = generator(input_text, max_length=150, num_beams=4, early_stopping=True)\n",
        "    result = generator(input_text, max_length=500, num_beams=5)\n",
        "\n",
        "\n",
        "    # Decode the generated text\n",
        "    answer = result[0]['generated_text']\n",
        "\n",
        "    end = time.time()\n",
        "    # print(f\"Context: {context}\")\n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"Answer: {result}\")\n",
        "    print(f\"runtime: {end - begin}\")\n",
        "    print(\"#########################\")"
      ],
      "metadata": {
        "id": "mwFlB4Br2R8-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Step-1 Load PDF\n",
        "    # pdf_docs = [\"Ishika_Garg_resume.pdf\"]\n",
        "    # pdf_docs = [\"NIPS-2017-attention-is-all-you-need-Paper.pdf\"]\n",
        "    pdf_docs = [\"HPOODataSheet.pdf\"]\n",
        "    raw_text = get_pdf_text(pdf_docs)\n",
        "\n",
        "    # Step-2: Clean the raw text\n",
        "    clean_text = clean_context(raw_text)\n",
        "\n",
        "    # Step-3: get the text chunks\n",
        "    text_chunks = get_text_chunks(clean_text)\n",
        "\n",
        "    # Step-4\n",
        "    vector_store = get_vectorstore(text_chunks)\n",
        "\n",
        "    # Step-5: Train model\n",
        "    # model_filename = 'PDF_extraction_model.pkl'\n",
        "    # if os.path.exists(model_filename):\n",
        "    #     with open(model_filename, 'rb') as file:\n",
        "    #         generator = pickle.load(file)\n",
        "    #     print(\"Model loaded successfully!\")\n",
        "    # else:\n",
        "    #     model_name = \"t5-large\"\n",
        "    #     # model_name = \"t5-small\"\n",
        "    #     generator = train_model(model_name)\n",
        "    #     with open(model_filename, 'wb') as file:\n",
        "    #         pickle.dump(generator, file)\n",
        "    #     print(\"Model saved successfully!\")\n",
        "    model_name = \"t5-large\"\n",
        "    generator = train_model(model_name)\n",
        "\n",
        "    # Step-6\n",
        "    while True:\n",
        "        question = input(\"Question (type 'exit' to quit): \")\n",
        "        if question.lower() == 'exit':\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "        else:\n",
        "            step6(vector_store, generator, question)"
      ],
      "metadata": {
        "id": "D-90olnf20vw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}