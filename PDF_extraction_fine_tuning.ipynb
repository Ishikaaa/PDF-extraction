{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOSVluyypSbdEzYud9K8t3y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ishikaaa/PDF-extraction/blob/main/PDF_extraction_fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2\n",
        "!pip install langchain\n",
        "!pip install InstructorEmbedding\n",
        "!pip install sentence-transformers==2.2.2\n",
        "!pip install faiss-gpu\n",
        "!pip install -U langchain-community\n",
        "# !pip install datasets\n",
        "# !pip install transformers\n",
        "# !pip install accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnfmvySYuHuQ",
        "outputId": "582aaea0-a7ee-42d1-fe5d-4378fd35f672"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.2.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.5)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.1)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.76)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.7.3)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain) (23.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.4)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.18.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.6.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain) (3.0.0)\n",
            "Requirement already satisfied: InstructorEmbedding in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: sentence-transformers==2.2.2 in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (4.41.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (4.66.4)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (2.3.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (0.18.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (1.11.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (0.23.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (4.12.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.6.0->sentence-transformers==2.2.2) (12.5.40)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.4.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers==2.2.2) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers==2.2.2) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers==2.2.2) (3.5.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers==2.2.2) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers==2.2.2) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers==2.2.2) (1.3.0)\n",
            "Requirement already satisfied: faiss-gpu in /usr/local/lib/python3.10/dist-packages (1.7.2)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.10/dist-packages (0.2.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.9.5)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: langchain<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.3)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.5)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.1.76)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.25.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (8.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.21.3)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.0->langchain-community) (0.2.1)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.0->langchain-community) (2.7.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain-community) (23.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (3.10.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2024.6.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (4.12.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain-community) (2.18.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OLogMKUruBNU"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "# from langchain.llms import HuggingFacePipeline\n",
        "# from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, pipeline\n",
        "import re\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "\n",
        "# Step-1: get text from single OR multiple PDFs\n",
        "def get_pdf_text(pdf_docs):\n",
        "    \"\"\"\n",
        "    args:\n",
        "        pdf_docs: list of pdfs\n",
        "    \"\"\"\n",
        "    text = \"\"\n",
        "\n",
        "    # iterate through all pdfs\n",
        "    for pdf in pdf_docs:\n",
        "        pdf_reader = PdfReader(pdf)\n",
        "        # iterate through all pages\n",
        "        for page in pdf_reader.pages:\n",
        "            text += page.extract_text()\n",
        "\n",
        "    return text\n",
        "\n",
        "def clean_context(context):\n",
        "    # Remove non-text elements\n",
        "    context = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', '', context)  # Remove email addresses\n",
        "    context = re.sub(r'\\bhttps?:\\/\\/\\S+\\b', '', context)  # Remove URLs\n",
        "    context = re.sub(r'\\b\\d+\\b', '', context)  # Remove standalone digits\n",
        "    # Add more regex patterns to remove other non-text elements\n",
        "\n",
        "    # Normalize whitespace\n",
        "    context = re.sub(r'\\s+', ' ', context)\n",
        "\n",
        "    # Lowercase text\n",
        "    context = context.lower()\n",
        "\n",
        "    return context\n",
        "\n",
        "# Step-2: get the text chunks\n",
        "def get_text_chunks(text):\n",
        "    \"\"\"\n",
        "    : return\n",
        "        a list of chunks of text that we will feed to our model\n",
        "    \"\"\"\n",
        "\n",
        "    text_splitter = CharacterTextSplitter(\n",
        "        separator=\"\\n\",\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=200,\n",
        "        length_function=len,\n",
        "    )\n",
        "    chunks = text_splitter.split_text(text)\n",
        "    return chunks\n",
        "\n",
        "\n",
        "# Step-3: Create Vector store\n",
        "def get_vectorstore(text_chunks):\n",
        "    # instructor embeddings\n",
        "    embeddings = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-xl\")\n",
        "    vectorstore = FAISS.from_texts(texts=text_chunks, embedding=embeddings)\n",
        "    return vectorstore\n",
        "\n",
        "\n",
        "def train_model(model_name):\n",
        "    model_name = \"t5-large\"\n",
        "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "    # Create a pipeline for text2text generation\n",
        "    generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "    return generator\n",
        "\n",
        "\n",
        "def step5(vector_store, generator, questions):\n",
        "    for question in questions:\n",
        "        docs = vector_store.similarity_search(question)\n",
        "        context = docs[0].page_content\n",
        "\n",
        "        # Combine the question and context for T5\n",
        "        input_text = f\"question: {question} context: {context}. Provide a detailed answer.\"\n",
        "\n",
        "        # Generate the answer with specific parameters\n",
        "        result = generator(input_text, max_length=150, num_beams=4, early_stopping=True)\n",
        "\n",
        "        # Decode the generated text\n",
        "        answer = result[0]['generated_text']\n",
        "\n",
        "        # print(f\"Context: {context}\")\n",
        "        print(f\"Question: {question}\")\n",
        "        print(f\"Answer: {answer}\")"
      ],
      "metadata": {
        "id": "iH0wpV2auDBV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "if __name__ == \"__main__\":\n",
        "    # Step-1 Load PDF\n",
        "    pdf_docs = [\"NIPS-2017-attention-is-all-you-need-Paper.pdf\"]\n",
        "    raw_text = get_pdf_text(pdf_docs)\n",
        "\n",
        "    clean_text = clean_context(raw_text)\n",
        "\n",
        "    # Step-2: get the text chunks\n",
        "    text_chunks = get_text_chunks(clean_text)\n",
        "\n",
        "\n",
        "    # Step-3\n",
        "    vector_store = get_vectorstore(text_chunks)\n",
        "    # print(\"text_chunks: \", vector_store)\n",
        "\n",
        "    # Step-4\n",
        "    model_filename = 'PDF_extraction_model.pkl'\n",
        "    if os.path.exists(model_filename):\n",
        "        with open(model_filename, 'rb') as file:\n",
        "            generator = pickle.load(file)\n",
        "        print(\"Model loaded successfully!\")\n",
        "    else:\n",
        "        model_name = \"t5-large\"\n",
        "        # model_name = \"t5-small\"\n",
        "        generator = train_model(model_name)\n",
        "        with open(model_filename, 'wb') as file:\n",
        "            pickle.dump(generator, file)\n",
        "        print(\"Model saved successfully!\")\n",
        "\n",
        "    # # Step-5\n",
        "    # questions = [\"Who is Aidan N. Gomez\", \"What is encoder and decoder?\", \"What did Ying Cao do?\"]\n",
        "    # step5(vector_store, generator, questions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "49vW5C7EuLje",
        "outputId": "b4ac1958-4e22-49b8-9b13-caad14674b26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load INSTRUCTOR_Transformer\n",
            "max_seq_length  512\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def step5(vector_store, generator, questions):\n",
        "    for question in questions:\n",
        "        begin = time.time()\n",
        "        docs = vector_store.similarity_search(question)\n",
        "        # context = docs[0].page_content\n",
        "        context = \" \".join([doc.page_content for doc in docs])\n",
        "\n",
        "        # Combine the question and context for T5\n",
        "        input_text = f\"question: {question} context: {context}. Provide a detailed answer.\"\n",
        "\n",
        "        # Generate the answer with specific parameters\n",
        "        # result = generator(input_text, max_length=150, num_beams=4, early_stopping=True)\n",
        "        result = generator(input_text, max_length=150, num_beams=4)\n",
        "\n",
        "        # Decode the generated text\n",
        "        answer = result[0]['generated_text']\n",
        "\n",
        "        end = time.time()\n",
        "        # print(f\"Context: {context}\")\n",
        "        print(f\"Question: {question}\")\n",
        "        print(f\"Answer: {answer}\")\n",
        "        print(f\"runtime: {end - begin}\")\n",
        "        print(\"#########################\")\n",
        "\n",
        "# while True:\n",
        "#     question = input(\"Question (type 'exit' to quit): \")\n",
        "#     if question.lower() == 'exit':\n",
        "#         print(\"Goodbye!\")\n",
        "#         break\n",
        "#     else:\n",
        "#         step5(vector_store, generator, question)\n",
        "questions = [\"Who is Aidan N. Gomez?\", \"Who is Noam Shazeer?\", \"What is encoder and decoder stacks?\", \"Who wrote Journal of Machine Learning Research?\", \"Who all were there in International Conference on Learning Representations?\", \"What is encoder?\"]\n",
        "# questions = [\"Who all were there in International Conference on Learning Representations?\"]\n",
        "step5(vector_store, generator, questions)\n",
        "# 55.93510174751282"
      ],
      "metadata": {
        "id": "5LuKGgEouVtc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vector_store)"
      ],
      "metadata": {
        "id": "hdRLiWme_Vcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def step5(vector_store, generator, questions):\n",
        "    for question in questions:\n",
        "        begin = time.time()\n",
        "        docs = vector_store.similarity_search(question)\n",
        "        # context = docs[0].page_content\n",
        "        context = \" \".join([doc.page_content for doc in docs])\n",
        "\n",
        "        # Combine the question and context for T5\n",
        "        input_text = f\"question: {question} context: {context}. Provide a detailed answer.\"\n",
        "\n",
        "        # Generate the answer with specific parameters\n",
        "        # result = generator(input_text, max_length=150, num_beams=4, early_stopping=True)\n",
        "        result = generator(input_text, max_length=150, num_beams=5)\n",
        "\n",
        "        # Decode the generated text\n",
        "        answer = result[0]['generated_text']\n",
        "\n",
        "        end = time.time()\n",
        "        # print(f\"Context: {context}\")\n",
        "        print(f\"Question: {question}\")\n",
        "        print(f\"Answer: {answer}\")\n",
        "        print(f\"runtime: {end - begin}\")\n",
        "        print(\"#########################\")\n",
        "\n",
        "# while True:\n",
        "#     question = input(\"Question (type 'exit' to quit): \")\n",
        "#     if question.lower() == 'exit':\n",
        "#         print(\"Goodbye!\")\n",
        "#         break\n",
        "#     else:\n",
        "#         step5(vector_store, generator, question)\n",
        "questions = [\"Who is Aidan N. Gomez?\", \"Who is Noam Shazeer?\", \"What is encoder and decoder stacks?\", \"Who wrote Journal of Machine Learning Research?\", \"Who all were there in International Conference on Learning Representations?\", \"What is encoder?\"]\n",
        "# questions = [\"Who all were there in International Conference on Learning Representations?\"]\n",
        "step5(vector_store, generator, questions)\n",
        "# 55.93510174751282"
      ],
      "metadata": {
        "id": "ientmkyn8a6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def step5(vector_store, generator, questions):\n",
        "    for question in questions:\n",
        "        begin = time.time()\n",
        "        docs = vector_store.similarity_search(question)\n",
        "        # context = docs[0].page_content\n",
        "        context = \" \".join([doc.page_content for doc in docs])\n",
        "\n",
        "        # Combine the question and context for T5\n",
        "        input_text = f\"question: {question} context: {context}. Provide a detailed answer.\"\n",
        "\n",
        "        # Generate the answer with specific parameters\n",
        "        result = generator(input_text, max_length=150, num_beams=4, early_stopping=True)\n",
        "        # result = generator(input_text, max_length=150, num_beams=4)\n",
        "\n",
        "        # Decode the generated text\n",
        "        answer = result[0]['generated_text']\n",
        "\n",
        "        end = time.time()\n",
        "        # print(f\"Context: {context}\")\n",
        "        print(f\"Question: {question}\")\n",
        "        print(f\"Answer: {answer}\")\n",
        "        print(f\"runtime: {end - begin}\")\n",
        "        print(\"#########################\")\n",
        "\n",
        "# while True:\n",
        "#     question = input(\"Question (type 'exit' to quit): \")\n",
        "#     if question.lower() == 'exit':\n",
        "#         print(\"Goodbye!\")\n",
        "#         break\n",
        "#     else:\n",
        "#         step5(vector_store, generator, question)\n",
        "questions = [\"Who is Aidan N. Gomez?\", \"Who is Noam Shazeer?\", \"What is encoder and decoder stacks?\", \"Who wrote Journal of Machine Learning Research?\", \"Who all were there in International Conference on Learning Representations?\", \"What is encoder?\"]\n",
        "# questions = [\"Who all were there in International Conference on Learning Representations?\"]\n",
        "step5(vector_store, generator, questions)\n",
        "# 55.93510174751282"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpVjkfqF3Xs9",
        "outputId": "a5700c3a-ad63-4ece-e49e-13ca29167a61"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: Who is Aidan N. Gomez?\n",
            "Answer: University of Toronto aidan@cs.toronto.edu\n",
            "runtime: 52.0031213760376\n",
            "#########################\n",
            "Question: Who is Noam Shazee?\n",
            "Answer: Noam Shazeer\n",
            "runtime: 42.0838737487793\n",
            "#########################\n",
            "Question: What is encoder and decoder stacks?\n",
            "Answer: N= 6 identical layers\n",
            "runtime: 35.78837966918945\n",
            "#########################\n",
            "Question: Who wrote Journal of Machine Learning Research?\n",
            "Answer: Jonas Gehring\n",
            "runtime: 46.74137854576111\n",
            "#########################\n",
            "Question: Who all were there in International Conference on Learning Representations?\n",
            "Answer: ukasz Kaiser and Ilya Sutskever\n",
            "runtime: 53.18675422668457\n",
            "#########################\n",
            "Question: What is encoder?\n",
            "Answer: maps an input sequence of symbol representations\n",
            "runtime: 37.842244386672974\n",
            "#########################\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def step5(vector_store, generator, questions):\n",
        "    # for question in questions:\n",
        "    begin = time.time()\n",
        "    docs = vector_store.similarity_search(questions)\n",
        "    # context = docs[0].page_content\n",
        "    context = \" \".join([doc.page_content for doc in docs])\n",
        "\n",
        "    # Combine the question and context for T5\n",
        "    input_text = f\"question: {question} context: {context}. Provide a detailed answer.\"\n",
        "\n",
        "    # Generate the answer with specific parameters\n",
        "    # result = generator(input_text, max_length=150, num_beams=4, early_stopping=True)\n",
        "    result = generator(input_text, max_length=150, num_beams=4)\n",
        "\n",
        "    # Decode the generated text\n",
        "    answer = result[0]['generated_text']\n",
        "\n",
        "    end = time.time()\n",
        "    # print(f\"Context: {context}\")\n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"Answer: {answer}\")\n",
        "    print(f\"runtime: {end - begin}\")\n",
        "    print(\"#########################\")\n",
        "\n",
        "while True:\n",
        "    question = input(\"Question (type 'exit' to quit): \")\n",
        "    if question.lower() == 'exit':\n",
        "        print(\"Goodbye!\")\n",
        "        break\n",
        "    else:\n",
        "        step5(vector_store, generator, question)\n",
        "# questions = [\"Who is Aidan N. Gomez\", \"What is encoder and decoder stacks?\", \"Who wrote Journal of Machine Learning Research?\", \"Who all were there in International Conference on Learning Representations?\", \"What is encoder?\"]\n",
        "# questions = [\"Who all were there in International Conference on Learning Representations?\"]\n",
        "# step5(vector_store, generator, questions)\n",
        "# 55.93510174751282"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSGUL3EOzbfH",
        "outputId": "9bf666bc-1978-4457-d6fc-2295e2c3f822"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question (type 'exit' to quit): Who is Noam Shazee\n",
            "Question: Who is Noam Shazee\n",
            "Answer: Ashish Vaswani Google Brain avaswani@google.comNoam Shazeer Google Brain noam@google.comNiki Parmar Google Research nikip@google.com\n",
            "runtime: 105.4311945438385\n",
            "#########################\n",
            "Question (type 'exit' to quit): Who is Noam Shazee?\n",
            "Question: Who is Noam Shazee?\n",
            "Answer: Noam Shazeer Google Brain noam@google.comNiki Parmar Google Research nikip@google.comAidan N. Gomezy University of Toronto aidan@cs.toronto.edu\n",
            "runtime: 88.47019076347351\n",
            "#########################\n",
            "Question (type 'exit' to quit): Who is Illia Polosukhin?\n",
            "Question: Who is Illia Polosukhin?\n",
            "Answer: illia.polosukhin@gmail.com\n",
            "runtime: 58.511088848114014\n",
            "#########################\n",
            "Question (type 'exit' to quit): exit\n",
            "Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Answer: noam@google.comNiki Parmar Google Research nikip@google.comAidan N. Gomezy University of Toronto aidan@cs.toronto.edu\n",
        "# runtime: 95.47286462783813\n",
        "# Answer: illia.polosukhin@gmail.com\n",
        "# runtime: 54.402446031570435\n",
        "# Answer: Noam Shazeer Google Brain noam@google.comNiki Parmar Google Research nikip@google.comAidan N. Gomezy University of Toronto aidan@cs.toronto.edu\n",
        "# runtime: 112.03104734420776\n",
        "# Question (type 'exit' to quit): Who is Aidan N. Gomez?\n",
        "# Answer: University of Toronto aidan@cs.toronto.edu\n",
        "# runtime: 73.178555727005"
      ],
      "metadata": {
        "id": "2MHnYn5lvf_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## num_beans = 4\n",
        "# #######################\n",
        "# Question: Who is Aidan N. Gomez\n",
        "# Answer: University of Toronto aidan@cs.toronto.edu\n",
        "# runtime: 70.98402190208435\n",
        "# #######################\n",
        "# Question: What is encoder and decoder stacks?\n",
        "# Answer: point-wise, fully connected layersWHo is\n",
        "# runtime: 38.34173512458801\n",
        "# #######################\n",
        "# Question: Who wrote Journal of Machine Learning Research?\n",
        "# Answer: Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin\n",
        "# runtime: 53.56891655921936\n",
        "# #######################\n",
        "# Question: Who all were there in International Conference on Learning Representations?\n",
        "# Answer: ukasz Kaiser and Ilya Sutskever\n",
        "# runtime: 55.72374773025513\n",
        "\n",
        "# ## num_beans = 5\n",
        "# #######################\n",
        "# Question: Who is Aidan N. Gomez\n",
        "# Answer: University of Toronto aidan@cs.toronto.edu\n",
        "# runtime: 80.05771541595459\n",
        "# #######################\n",
        "# Question: What is encoder and decoder stacks?\n",
        "# Answer: point-wise, fully connected layers\n",
        "# runtime: 53.74897265434265\n",
        "# #######################\n",
        "# Question: Who wrote Journal of Machine Learning Research?\n",
        "# Answer: Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin\n",
        "# runtime: 59.80621600151062\n",
        "# #######################\n",
        "# Question: Who all were there in International Conference on Learning Representations?\n",
        "# Answer: ukasz Kaiser and Ilya Sutskever\n",
        "# runtime: 73.25919771194458\n",
        "# #######################\n",
        "# Question: What is encoder?\n",
        "# Answer: maps an input sequence of symbol representations (x1;:::;x n)to a sequence of continuous representations z= (z1;:::;z m)\n",
        "# runtime: 60.467962980270386\n"
      ],
      "metadata": {
        "id": "pyPi8mUgims-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}